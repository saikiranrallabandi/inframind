# IAPO: Infrastructure-Aware Policy Optimization

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![arXiv](https://img.shields.io/badge/arXiv-2024.XXXXX-b31b1b.svg)](https://arxiv.org/)

**The first reinforcement learning framework for training small language models to reason about Infrastructure-as-Code.**

## The Problem

Large Language Models (GPT-4, Claude) can generate Infrastructure-as-Code, but:
- **Cost**: API calls add up ($100s-$1000s/month for teams)
- **Privacy**: Your infrastructure code is sent to external servers
- **Offline**: Doesn't work in air-gapped/secure environments
- **Customization**: Can't fine-tune on your specific patterns

Small open-source models (< 1B parameters) fail at IaC because:
- They **hallucinate** resource names (`aws_ec2` instead of `aws_instance`)
- They generate **invalid syntax** that won't pass `terraform validate`
- They **ignore security** best practices
- Traditional fine-tuning (SFT/LoRA) only **memorizes patterns**, doesn't teach reasoning

## Our Solution

**IAPO** (Infrastructure-Aware Policy Optimization) uses reinforcement learning to teach small models to **reason** about infrastructure, not just memorize examples.

| Approach | Method | Result |
|----------|--------|--------|
| SFT/LoRA | "Memorize this Terraform example" | Copies patterns, fails on novel tasks |
| **IAPO** | "Generate Terraform, I'll score if it's valid" | Learns reasoning, handles new tasks |

### Key Innovation

IAPO extends GRPO (Group Relative Policy Optimization) with **infrastructure-specific rewards**:

```
IAPO Reward = Î± Ã— Syntax + Î² Ã— Correctness + Î³ Ã— Format

Where:
- Syntax: Does it pass `terraform validate`?
- Correctness: Are the right resources used?
- Format: Is the structure proper?
```

## Features

- ğŸš€ **IaC-Bench**: 500+ tasks across Terraform, Kubernetes, Docker, CI/CD
- ğŸ§  **GRPO Training**: Reinforcement learning that teaches reasoning
- ğŸ”§ **Qwen-based**: Strong base model for infrastructure tasks
- ğŸ“¦ **Alpaca Format**: Compatible with standard training pipelines
- ğŸ  **Local-first**: Runs entirely on your machine

## Installation

```bash
pip install iapo
```

Or from source:

```bash
git clone https://github.com/saikiranrallabandi/iapo.git
cd iapo
pip install -e .
```

## Quick Start

```python
from iapo import create_dataset, IAPOTrainer

# Load 500+ IaC tasks
dataset = create_dataset(size=100)

# Train with IAPO (GRPO + IaC rewards)
trainer = IAPOTrainer(model_name="Qwen/Qwen2.5-0.5B-Instruct")
trainer.train(dataset, epochs=1)

# Save your IaC-reasoning model
trainer.save("./my-iac-model")
```

## How It Works

### 1. IaC-Bench Dataset

529 infrastructure tasks in [Alpaca format](https://github.com/tatsu-lab/stanford_alpaca):

```json
{
  "instruction": "Create Terraform for AWS EC2 instance",
  "input": "t2.micro instance type",
  "output": ""
}
```

| Category | Tasks | Examples |
|----------|-------|----------|
| Terraform | 225 | EC2, S3, VPC, RDS, EKS, Lambda, IAM |
| Kubernetes | 138 | Deployments, Services, Ingress, RBAC |
| Docker | 70 | Dockerfiles, docker-compose |
| CI/CD | 96 | GitHub Actions, GitLab CI, Jenkins |

### 2. IAPO Training Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IAPO TRAINING                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  For each IaC task:                                            â”‚
â”‚                                                                 â”‚
â”‚  1. GENERATE: Model produces multiple IaC outputs              â”‚
â”‚     "Create EC2" â†’ [output1, output2]                          â”‚
â”‚                                                                 â”‚
â”‚  2. SCORE: IAPO reward function evaluates each                 â”‚
â”‚     output1: syntax=1.0, correct=0.8, format=0.9 â†’ 0.89        â”‚
â”‚     output2: syntax=0.0, correct=0.5, format=0.7 â†’ 0.38        â”‚
â”‚                                                                 â”‚
â”‚  3. ADVANTAGE: Compare within group (GRPO)                     â”‚
â”‚     output1: above average â†’ positive advantage                â”‚
â”‚     output2: below average â†’ negative advantage                â”‚
â”‚                                                                 â”‚
â”‚  4. UPDATE: Increase probability of better outputs             â”‚
â”‚     Model learns: "valid syntax = higher reward"               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. IAPO Reward Function

```python
from iapo import IaCReward

reward = IaCReward(alpha=0.4, beta=0.3, gamma=0.3)

# Score a Terraform output
score, details = reward.score(terraform_code, category="terraform")
# score: 0.85
# details: {"syntax": 1.0, "correctness": 0.8, "format": 0.75}
```

**Reward components:**

| Component | Weight | What it measures |
|-----------|--------|------------------|
| Syntax | 0.4 | Valid resource declarations |
| Correctness | 0.3 | Right resource types used |
| Format | 0.3 | Proper structure (balanced braces, etc.) |

## Training

### Basic Training

```bash
python scripts/train.py --epochs 3 --output ./iapo-model
```

### Category-Specific Training

```bash
# Train only on Terraform
python scripts/train.py --category terraform --epochs 5

# Train only on Kubernetes
python scripts/train.py --category kubernetes --epochs 5
```

### Custom Training

```python
from iapo import create_dataset, IAPOTrainer

# Load specific categories
dataset = create_dataset(categories=["terraform", "kubernetes"], size=200)

# Configure trainer
trainer = IAPOTrainer(
    model_name="Qwen/Qwen2.5-0.5B-Instruct",
    lr=1e-5,
    group_size=4  # More samples per task for better GRPO
)

# Train
history = trainer.train(dataset, epochs=3)

# Check progress
for epoch in history:
    print(f"Epoch {epoch['epoch']}: Reward = {epoch['mean_reward']:.3f}")
```

## Comparison with Existing Work

| Project | Method | Model Size | IaC-specific | Open Source |
|---------|--------|------------|--------------|-------------|
| [devops-slm-v1](https://huggingface.co/lakhera2023/devops-slm-v1) | LoRA/SFT | 907M | âœ… | âœ… |
| [AIAC](https://github.com/gofireflyio/aiac) | Prompting | API-based | âœ… | âœ… |
| GPT-4 / Claude | - | >100B | âŒ | âŒ |
| **IAPO** | **GRPO/RL** | **0.5B-3B** | **âœ…** | **âœ…** |

**Key differentiator**: IAPO uses **reinforcement learning** with infrastructure-specific rewards, enabling models to **reason** about IaC rather than just memorize patterns.

## Project Structure

```
iapo/
â”œâ”€â”€ iapo/
â”‚   â”œâ”€â”€ __init__.py      # Package exports
â”‚   â”œâ”€â”€ dataset.py       # IaC-Bench (500+ tasks)
â”‚   â”œâ”€â”€ rewards.py       # IAPO reward functions
â”‚   â””â”€â”€ train.py         # GRPO trainer
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.py         # Training CLI
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ quickstart.py    # Quick start example
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ pyproject.toml
```

## Roadmap

- [x] IaC-Bench dataset (500+ tasks)
- [x] IAPO trainer with GRPO
- [x] Basic reward functions
- [ ] Pre-trained model release on HuggingFace
- [ ] Real validation integration (`terraform validate`)
- [ ] Security scoring (`tfsec`, `checkov`)
- [ ] CLI tool (`iapo generate "create S3 bucket"`)
- [ ] VS Code extension

## Citation

```bibtex
@article{rallabandi2024iapo,
  title={IAPO: Infrastructure-Aware Policy Optimization for Small Language Models},
  author={Rallabandi, Sai Kiran},
  journal={arXiv preprint arXiv:2024.XXXXX},
  year={2024}
}
```

## Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- [DeepSeek](https://github.com/deepseek-ai) for GRPO
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) for data format
- [Reasoning Gym](https://github.com/open-thought/reasoning-gym) for architecture inspiration
- [AIAC](https://github.com/gofireflyio/aiac) for IaC task patterns
